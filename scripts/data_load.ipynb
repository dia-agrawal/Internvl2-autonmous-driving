{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e97cab",
   "metadata": {},
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset 'OpenDriveLab/DriveLM' is a gated dataset on the Hub. You must be authenticated to access it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_DATASETS_CACHE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/vipin/dia/selfdrive/InternVL/data/driveLM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 6\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOpenDriveLab/DriveLM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/internvl/lib/python3.10/site-packages/datasets/load.py:1397\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   1393\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1394\u001b[0m )\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1397\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/envs/internvl/lib/python3.10/site-packages/datasets/load.py:1137\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m     features \u001b[38;5;241m=\u001b[39m _fix_for_backward_compatible_features(features)\n\u001b[0;32m-> 1137\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m builder_kwargs \u001b[38;5;241m=\u001b[39m dataset_module\u001b[38;5;241m.\u001b[39mbuilder_kwargs\n",
      "File \u001b[0;32m~/miniconda3/envs/internvl/lib/python3.10/site-packages/datasets/load.py:1030\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach the Hugging Face Hub for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\n\u001b[0;32m-> 1030\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1035\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/internvl/lib/python3.10/site-packages/datasets/load.py:1016\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m   1015\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Visit the dataset page at https://huggingface.co/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to ask for access.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetNotFoundError(\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRevision \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist for dataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'OpenDriveLab/DriveLM' is a gated dataset on the Hub. You must be authenticated to access it."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/home/vipin/dia/selfdrive/InternVL/data\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"/home/vipin/dia/selfdrive/InternVL/data/driveLM\"\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"OpenDriveLab/DriveLM\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478aafee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/solesensei/solesensei_bdd100k?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.61G/7.61G [02:42<00:00, 50.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/vipin/.cache/kagglehub/datasets/solesensei/solesensei_bdd100k/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"solesensei/solesensei_bdd100k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73c8d292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded items: 69863\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_jsonl, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fo:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[0;32m---> 32\u001b[0m         img_name \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# e.g. \"0a0c3694-9572f64f.jpg\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m         caption \u001b[38;5;241m=\u001b[39m describe_scene(item)\n\u001b[1;32m     35\u001b[0m         entry \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: idx,\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m: img_name,   \u001b[38;5;66;03m# filename only\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m             ],\n\u001b[1;32m     42\u001b[0m         }\n",
      "\u001b[0;31mKeyError\u001b[0m: 'name'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_json = \"data/bdd100k/train_internvl_chat.jsonl\"\n",
    "output_jsonl = \"data/bdd100k/train_internvl_chat1.jsonl\"\n",
    "\n",
    "def describe_scene(item):\n",
    "    attrs = item.get(\"attributes\", {})\n",
    "    weather = attrs.get(\"weather\", \"unknown weather\")\n",
    "    scene = attrs.get(\"scene\", \"road\")\n",
    "    tod = attrs.get(\"timeofday\", \"daytime\")\n",
    "\n",
    "    labels = item.get(\"labels\", [])\n",
    "    objs = [lab.get(\"category\", \"object\") for lab in labels]\n",
    "    obj_description = \", \".join(objs) if objs else \"no visible objects\"\n",
    "\n",
    "    return f\"A {weather} {scene} during {tod}. The scene contains: {obj_description}.\"\n",
    "\n",
    "# ---- read labels as JSONL or concatenated JSON ----\n",
    "data = []\n",
    "with open(input_json, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(\"Loaded items:\", len(data))\n",
    "\n",
    "# ---- write InternVL chat jsonl ----\n",
    "with open(output_jsonl, \"w\") as fo:\n",
    "    for idx, item in enumerate(data):\n",
    "        img_name = item[\"name\"]  # e.g. \"0a0c3694-9572f64f.jpg\"\n",
    "        caption = describe_scene(item)\n",
    "\n",
    "        entry = {\n",
    "            \"id\": idx,\n",
    "            \"image\": img_name,   # filename only\n",
    "            \"conversations\": [\n",
    "                {\"from\": \"human\", \"value\": \"<image>\\nDescribe this driving scene.\"},\n",
    "                {\"from\": \"gpt\",   \"value\": caption},\n",
    "            ],\n",
    "        }\n",
    "        fo.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", output_jsonl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1baeba76",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/DriveLM/drivelm_val.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/DriveLM/drivelm_val.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m      \u001b[38;5;66;03m# your current file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m outp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/DriveLM/drivelm_val_fixed.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fi, \u001b[38;5;28mopen\u001b[39m(outp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fo:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fi:\n\u001b[1;32m      8\u001b[0m         j \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/internvl/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/DriveLM/drivelm_val.jsonl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "inp = \"data/DriveLM/drivelm_val.jsonl\"      # your current file\n",
    "outp = \"data/DriveLM/drivelm_val_fixed.jsonl\"\n",
    "\n",
    "with open(inp) as fi, open(outp, \"w\") as fo:\n",
    "    for line in fi:\n",
    "        j = json.loads(line)\n",
    "        img = j[\"image\"]\n",
    "        img = img.replace(\"../nuscenes/\", \"nuscenes/\")  # drop the \"..\"\n",
    "        j[\"image\"] = img\n",
    "        fo.write(json.dumps(j) + \"\\n\")\n",
    "\n",
    "print(\"wrote\", outp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60deac06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb13d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comprehensive DriveLM Data Check ===\n",
      "\n",
      "1. Directory structure in data/DriveLM/:\n",
      "  drivelm_nus_imgs_train (1 items)\n",
      "  drivelm_nus_imgs_val (1 items)\n",
      "  drivelm_train.jsonl (114829348 bytes)\n",
      "  drivelm_val.jsonl (12709390 bytes)\n",
      "  make_drivelm_internvl.py (2356 bytes)\n",
      "  v1_1_train_nus.json (192961829 bytes)\n",
      "  v1_1_val_nus_q_only.json (9860626 bytes)\n",
      "\n",
      "2. Check first record in drivelm_train.jsonl:\n",
      "  Full first record:\n",
      "{\n",
      "  \"image\": \"nuscenes/samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707819362404.jpg\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"user\",\n",
      "      \"value\": \"<img>\\nExcept for the ego vehicle, what object would consider <c5,CAM_BACK,1005.0,512.7> to be most relevant to its decision?\\n</img>\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"assistant\",\n",
      "      \"value\": \"None.\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "  Image field value: 'nuscenes/samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707819362404.jpg'\n",
      "  Image field type: <class 'str'>\n",
      "\n",
      "3. Try different path interpretations:\n",
      "  nuscenes/samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707819362404.jpg NOT FOUND\n",
      "  data/DriveLM/nuscenes/samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707819362404.jpg NOT FOUND\n",
      "  data/nuscenes/samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707819362404.jpg NOT FOUND\n",
      "  nuscenes/samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707819362404.jpg NOT FOUND\n",
      "  nuscenes/samples/CAM_FRONT/n008-2018-07-27-12-07-38-0400__CAM_FRONT__1532707819362404.jpg NOT FOUND\n",
      "\n",
      "4. Search for nuscenes images:\n",
      "  Found: data/DriveLM/drivelm_nus_imgs_train/nuscenes (24432 images)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Comprehensive DriveLM Data Check ===\\n\")\n",
    "\n",
    "# 1. Check what directories exist\n",
    "print(\"1. Directory structure in data/DriveLM/:\")\n",
    "drivelm_dir = Path(\"data/DriveLM\")\n",
    "if drivelm_dir.exists():\n",
    "    for item in sorted(drivelm_dir.iterdir())[:20]:\n",
    "        size = f\"({len(list(item.iterdir()))} items)\" if item.is_dir() else f\"({item.stat().st_size} bytes)\"\n",
    "        print(f\"  {item.name} {size}\")\n",
    "else:\n",
    "    print(f\"  Directory does not exist: {drivelm_dir.absolute()}\")\n",
    "\n",
    "print(\"\\n2. Check first record in drivelm_train.jsonl:\")\n",
    "drivelm_jsonl = \"data/DriveLM/drivelm_train.jsonl\"\n",
    "with open(drivelm_jsonl, 'r') as f:\n",
    "    first_record = json.loads(f.readline())\n",
    "    print(f\"  Full first record:\")\n",
    "    print(json.dumps(first_record, indent=2)[:500])\n",
    "    \n",
    "    img_field = first_record.get(\"image\", \"\")\n",
    "    print(f\"\\n  Image field value: '{img_field}'\")\n",
    "    print(f\"  Image field type: {type(img_field)}\")\n",
    "\n",
    "print(\"\\n3. Try different path interpretations:\")\n",
    "possible_paths = [\n",
    "    img_field,                                           # as-is\n",
    "    f\"data/DriveLM/{img_field}\",                         # relative from repo root\n",
    "    f\"data/{img_field}\",                                 # data/ + field value\n",
    "    img_field.lstrip(\"../\"),                             # strip leading ../\n",
    "    img_field.replace(\"../nuscenes/\", \"nuscenes/\"),    # like your fix script\n",
    "]\n",
    "\n",
    "for test_path in possible_paths:\n",
    "    exists = os.path.exists(test_path)\n",
    "    print(f\"  {test_path:<80} {'✓ EXISTS' if exists else 'NOT FOUND'}\")\n",
    "\n",
    "print(\"\\n4. Search for nuscenes images:\")\n",
    "nuscenes_dirs = list(Path(\"data/DriveLM\").rglob(\"**/nuscenes\"))\n",
    "if nuscenes_dirs:\n",
    "    for ndir in nuscenes_dirs[:5]:\n",
    "        img_count = len(list(ndir.glob(\"**/*.jpg\")))\n",
    "        print(f\"  Found: {ndir} ({img_count} images)\")\n",
    "else:\n",
    "    # Try a broader search\n",
    "    all_subdirs = list(Path(\"data/DriveLM\").rglob(\"*\"))\n",
    "    print(f\"  Total items in DriveLM: {len(all_subdirs)}\")\n",
    "    jpg_files = list(Path(\"data/DriveLM\").rglob(\"*.jpg\"))\n",
    "    print(f\"  JPG files found: {len(jpg_files)}\")\n",
    "    if jpg_files:\n",
    "        print(f\"  Sample image paths:\")\n",
    "        for jpg in jpg_files[:3]:\n",
    "            print(f\"    {jpg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f38113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kept 340159 skipped 0\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from PIL import Image\n",
    "\n",
    "root = \"data/DriveLM/drivelm_nus_imgs_train\"\n",
    "inp  = \"data/DriveLM/drivelm_train.jsonl\"\n",
    "outp = \"data/DriveLM/drivelm_chat_clean.jsonl\"\n",
    "\n",
    "good, bad = 0, 0\n",
    "with open(inp) as fi, open(outp, \"w\") as fo:\n",
    "    for line in fi:\n",
    "        j = json.loads(line)\n",
    "        path = os.path.join(root, j[\"image\"])\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            img.verify()\n",
    "            fo.write(json.dumps(j) + \"\\n\")\n",
    "            good += 1\n",
    "        except Exception:\n",
    "            bad += 1\n",
    "            if bad < 20:\n",
    "                print(\"BAD:\", path)\n",
    "\n",
    "print(\"kept\", good, \"skipped\", bad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe67437",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'internvl_chat/shell/data/drivelm_finetune.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minternvl_chat/shell/data/drivelm_finetune.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreplace_img\u001b[39m(obj):\n",
      "File \u001b[0;32m~/miniconda3/envs/internvl/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'internvl_chat/shell/data/drivelm_finetune.jsonl'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "path = \"internvl_chat/shell/data/drivelm_finetune.json\"\n",
    "count = 0\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def replace_img(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: replace_img(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [replace_img(x) for x in obj]\n",
    "    elif isinstance(obj, str):\n",
    "        return obj.replace(\"<img>\", \"<image>\")\n",
    "        count += 1\n",
    "    else:\n",
    "        return obj\n",
    "    print(\"Total replacements made:\", count)\n",
    "\n",
    "data = replace_img(data)\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    json.dump(data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ccd31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internvl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
